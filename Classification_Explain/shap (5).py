"""
SHAP explainers for VRDU models
===============================

This module provides a set of classes that wrap the SHAP library to explain
visual document understanding (VRDU) models trained on multimodal inputs
containing text, layout and vision information.  The goal is to produce
meaningful importance scores for each modality without relying on the ad hoc
implementation previously present in this repository.  The implementation
here is built from scratch using SHAP's recommended practices.

Why log–odds?
-------------

By default SHAP computes attributions in the same units as the model output.
For classification models this means probabilities.  However, probabilities
do not add linearly – doubling the probability of a class from 0.1 to 0.2 is
not the same “amount of evidence” as doubling from 0.4 to 0.8.  SHAP's
documentation recommends working in the log–odds (logit) space so that
Shapley values add and subtract bits of evidence【697382921012888†L2521-L2527】.
The recommended way to do this is either to convert probabilities to
log-odds before passing them to SHAP, or to supply a link function
(`shap.links.logit`) when constructing the explainer so that SHAP performs
the conversion internally【563834421952066†L100-L107】.  In the example of
defining a custom model for text classification, SHAP converts softmax
probabilities to logits (log-odds) before explanation【773134568126607†L88-L97】.

In this module we choose to keep the conversion explicit: when the
``use_logit`` flag is true the underlying model outputs probabilities and
SHAP uses a logit link to translate them into log–odds.  When ``use_logit``
is false we convert probabilities to log–odds ourselves and supply an
identity link.  This gives users flexibility while always working in an
additive space.

Classes provided
----------------

``BaseShapExplainer``
    Handles device placement, model evaluation and conversion of VRDU
    ``DocSample`` objects into model inputs via the supplied ``encode_fn``.

``SHAPTextExplainer``
    Computes token-level attributions for the textual modality.  Perturbations
    are generated by masking words with a special token and, optionally,
    collapsing bounding boxes.  The explainer uses
    ``shap.maskers.Text`` internally to construct the cooperative game.

``SHAPLayoutExplainer``
    Attributes importance to the layout (bounding boxes) by perturbing the
    spatial coordinates while keeping the words fixed.  A simple sentinel
    tokenizer is used to convert space‑delimited strings into tokens so that
    SHAP can build its hierarchy.

``SHAPVisionExplainer``
    Attributes importance to the image modality.  Pixels (or superpixels)
    are masked using ``shap.maskers.Image``, and the associated text and
    bounding boxes are kept fixed.

Usage
-----

The explainers expect a model callable from HuggingFace (e.g. LayoutLMv3 or
BROS), an ``encode_fn`` that maps a list of ``DocSample`` objects into
tokenized model inputs on the correct device, and an optional tokenizer.
Once instantiated, call ``explain(sample)`` on a ``DocSample`` to obtain
SHAP attributions for that sample.  A small example for text explanation:

>>> explainer = SHAPTextExplainer(model, encode_fn, tokenizer)
>>> shap_values = explainer.explain(doc_sample)

The returned value is a ``shap.Explanation`` object that can be visualised
with ``shap.plots.text`` or converted into a mapping of words to importance
scores.
"""

from __future__ import annotations

import numpy as np
import torch
import shap
from PIL import Image
from functools import wraps
from typing import Callable, List, Optional

from transformers import LayoutLMv3TokenizerFast

from vrdu_utils.module_types import DocSample

__all__ = [
    "make_layoutlmv3_tokenizer_wrapper",
    "BaseShapExplainer",
    "SHAPTextExplainer",
    "SHAPLayoutExplainer",
    "SHAPVisionExplainer",
]


def make_layoutlmv3_tokenizer_wrapper(
    tkn: LayoutLMv3TokenizerFast, dummy_box: tuple[int, int, int, int] = (0, 0, 0, 0)
) -> Callable:
    """Return a callable that always feeds LayoutLMv3 a ``(words, boxes)`` pair.

    The HuggingFace ``LayoutLMv3TokenizerFast`` expects either raw strings or
    tuples of ``(words, boxes)`` when ``is_split_into_words=True``.  When SHAP
    perturbs a document it may supply an empty string for the baseline input.
    This wrapper ensures that the tokenizer is always called with a list of
    tokens and an accompanying list of dummy bounding boxes to avoid tokeniser
    errors.  For non-layout tokenisers this wrapper is not needed.

    Args:
        tkn: The original ``LayoutLMv3TokenizerFast`` instance.
        dummy_box: A bounding box to assign to masked tokens when none is
            provided.  It defaults to the zero box.

    Returns:
        A wrapped tokenizer callable that accepts either a string or a list of
        strings and always passes ``is_split_into_words=True`` with a list of
        bounding boxes.
    """

    def _to_words(x: str | List[str]) -> List[str]:
        # If already a sequence of tokens, return as list; otherwise split on
        # whitespace.  This permits SHAP to provide either raw strings or lists.
        return list(x) if isinstance(x, (list, tuple)) else x.split()

    @wraps(tkn)
    def wrapped(texts, **kwargs):
        # Single example case
        if isinstance(texts, str):
            words = _to_words(texts)
            if "boxes" not in kwargs:
                kwargs["boxes"] = [dummy_box] * len(words)
            return tkn(words, is_split_into_words=True, **kwargs)
        # Batch case
        batch_words: List[List[str]] = [_to_words(s) for s in texts]
        if "boxes" not in kwargs:
            kwargs["boxes"] = [ [dummy_box] * len(seq) for seq in batch_words ]
        return tkn(batch_words, is_split_into_words=True, **kwargs)

    return wrapped


def sentinel_tokenizer(
    s: str, return_offsets_mapping: bool = True, **_: dict
) -> dict[str, List]:
    """A minimal tokenizer that treats whitespace‑separated words as tokens.

    This tokenizer conforms to the subset of the HuggingFace Transformers API
    required by ``shap.maskers.Text``: it returns a dictionary with an
    ``input_ids`` field (containing a list of tokens) and, optionally, an
    ``offset_mapping`` field (containing character ranges for each token).

    Args:
        s: The string to tokenize.
        return_offsets_mapping: If ``True`` (the default) the returned
            dictionary includes character offset ranges for each token.
        _: Additional unused keyword arguments to match the tokenizer API.

    Returns:
        A dictionary with ``input_ids`` and (optionally) ``offset_mapping``.
    """
    tokens = [t for t in s.split() if t]
    ids: List[str] = tokens.copy()  # identity mapping for words
    if not return_offsets_mapping:
        return {"input_ids": ids}
    # compute offsets
    offsets: List[tuple[int, int]] = []
    pos = 0
    for t in tokens:
        start = s.find(t, pos)
        end = start + len(t)
        offsets.append((start, end))
        pos = end
    return {"input_ids": ids, "offset_mapping": offsets}


class BaseShapExplainer:
    """Base class for SHAP explainers of VRDU models.

    This class handles device placement and exposes ``_predict`` and
    ``_encode`` methods that convert lists of ``DocSample`` objects into
    model outputs suitable for SHAP.  It is agnostic to the modality being
    explained; modality‑specific explainers derive from this class and
    implement ``_make_predict_fn`` and ``explain``.
    """

    def __init__(
        self,
        model: torch.nn.Module,
        encode_fn: Callable[[List[DocSample], torch.device], dict],
        *,
        algorithm: str = "partition",
        device: Optional[str] = None,
        use_logit: bool = True,
    ) -> None:
        # Put model in evaluation mode and record encode function
        self.model = model.eval()
        self.encode_fn = encode_fn
        # Determine device (use GPU if available by default)
        self.device = torch.device(device or ("cuda" if torch.cuda.is_available() else "cpu"))
        self.model.to(self.device)
        # Extract class names from the model config if available
        self.class_names = []
        if hasattr(model, "config") and getattr(model.config, "id2label", None):
            # Sort by index to ensure deterministic ordering
            id2label = model.config.id2label
            self.class_names = [id2label[k] for k in sorted(id2label)]
        # Algorithm for SHAP (partition works well for text)
        self.algorithm: str = algorithm
        # Whether to use SHAP's logit link (True) or convert to log odds manually
        self.use_logit: bool = use_logit

    def _encode(self, samples: List[DocSample]) -> dict:
        """Tokenize and prepare model inputs on the correct device."""
        return self.encode_fn(samples, self.device)

    @torch.no_grad()
    def _predict(self, samples: List[DocSample]) -> np.ndarray:
        """Compute model outputs for a batch of ``DocSample`` objects.

        If ``use_logit`` is true the returned values are class probabilities
        obtained via a softmax; SHAP will convert them to log‑odds via the
        provided link function.  Otherwise the returned values are log‑odds
        computed directly from the probabilities.
        """
        encoded = self._encode(samples)
        # Some transformers return a dict; handle both cases
        try:
            logits = self.model(**encoded).logits
        except Exception:
            logits = self.model(**encoded)["logits"]
        # Compute class probabilities
        probs = torch.softmax(logits, dim=-1)
        if self.use_logit:
            return probs.cpu().numpy()
        # Convert probabilities to log–odds manually.  Avoid division by zero.
        eps = 1e-15
        log_odds = torch.log(probs / torch.clamp(1.0 - probs, min=eps))
        return log_odds.cpu().numpy()

    def explain(self, *args, **kwargs):
        """Abstract method overridden by subclasses to return SHAP values."""
        raise NotImplementedError

    def explain_batch(self, samples: List[DocSample], **kwargs):
        """Explain a list of samples using the subclass's ``explain`` method."""
        return [self.explain(s, **kwargs) for s in samples]


class SHAPTextExplainer(BaseShapExplainer):
    """Explain token‑level importance of the text modality.

    The explainer builds a SHAP cooperative game over the words in a
    ``DocSample`` by masking them with a user‑specified ``mask_token``.
    Perturbed documents are converted back into ``DocSample`` objects and
    passed through the model to compute probabilities or logits.  SHAP then
    computes the attribution of each word.
    """

    def __init__(
        self,
        model: torch.nn.Module,
        encode_fn: Callable[[List[DocSample], torch.device], dict],
        tokenizer: Callable | LayoutLMv3TokenizerFast,
        *,
        mask_token: str = "[UNK]",
        device: Optional[str] = None,
        algorithm: str = "partition",
        batch_size: int = 16,
        use_logit: bool = True,
    ) -> None:
        super().__init__(model, encode_fn, algorithm=algorithm, device=device, use_logit=use_logit)
        # Wrap the tokenizer if necessary (LayoutLMv3 requires (words, boxes))
        if isinstance(tokenizer, LayoutLMv3TokenizerFast):
            self.tokenizer = make_layoutlmv3_tokenizer_wrapper(tokenizer)
        else:
            self.tokenizer = tokenizer
        self.mask_token = mask_token
        self.batch_size = batch_size

    def _batched_predict(self, samples: List[DocSample]) -> np.ndarray:
        """Apply ``_predict`` in batches to avoid out‑of‑memory errors."""
        outputs: List[np.ndarray] = []
        for i in range(0, len(samples), self.batch_size):
            batch = samples[i : i + self.batch_size]
            outputs.append(self._predict(batch))
        return np.vstack(outputs)

    def _make_predict_fn(self, sample: DocSample, align_boxes: bool = False) -> Callable:
        """Return a function that maps perturbed text strings to model outputs.

        The returned callable takes a list of whitespace‑delimited strings.  Each
        string is split into words and converted into a new ``DocSample`` by
        reusing the original image and bounding boxes from ``sample``.  When
        ``align_boxes`` is true, masked tokens receive a full page bounding
        box, effectively collapsing their spatial contribution; otherwise the
        original bounding boxes are kept.  The callable returns a matrix of
        model outputs, one row per perturbation.
        """
        width, height = sample.image.size
        dummy_box = [0, 0, width, height]

        def fn(texts: List[str]) -> np.ndarray:
            perturbed_samples: List[DocSample] = []
            for sent in texts:
                words = sent.split()  # SHAP supplies space‑separated words
                boxes: List = []
                for idx, w in enumerate(words):
                    if idx < len(sample.bboxes):
                        if align_boxes and w == self.mask_token:
                            boxes.append(dummy_box)
                        else:
                            boxes.append(sample.bboxes[idx])
                    else:
                        # If SHAP created more tokens than we started with,
                        # assign a dummy bounding box
                        boxes.append(dummy_box)
                perturbed_samples.append(
                    DocSample(
                        image=sample.image,
                        words=words,
                        bboxes=boxes,
                        ner_tags=sample.ner_tags,
                        label=sample.label,
                    )
                )
            return self._batched_predict(perturbed_samples)

        return fn

    def explain(
        self,
        sample: DocSample,
        *,
        align_boxes: bool = False,
        num_samples: int = 2000,
    ) -> shap.Explanation:
        """Compute SHAP values for one ``DocSample`` using a Text masker.

        Args:
            sample: The document to explain.
            align_boxes: If ``True`` masked words receive a dummy bounding box
                covering the entire page.  This can be useful when interpreting
                models that combine textual and layout information because it
                suppresses the influence of spatial features when a token is
                masked.
            num_samples: The maximum number of model evaluations SHAP will
                perform when estimating the Shapley values.

        Returns:
            A ``shap.Explanation`` containing attributions for each word in
            ``sample.words`` across all output classes.
        """
        predict_fn = self._make_predict_fn(sample, align_boxes=align_boxes)
        # Choose a link function based on whether we output probabilities or
        # logits.  When ``use_logit`` is true we return probabilities and ask
        # SHAP to convert them to log–odds; otherwise we return log–odds and
        # leave the link as identity【563834421952066†L100-L107】.
        link_fn = shap.links.logit if self.use_logit else shap.links.identity
        masker = shap.maskers.Text(
            tokenizer=self.tokenizer,
            mask_token=self.mask_token,
            collapse_mask_token="auto",
        )
        explainer = shap.Explainer(
            predict_fn,
            masker=masker,
            link=link_fn,
            algorithm=self.algorithm,
            output_names=self.class_names or None,
        )
        # Build the raw document string from the sample's words.  SHAP uses
        # whitespace to delimit features.  We must wrap in a list because
        # SHAP expects a batch of inputs.
        document = " ".join(sample.words)
        explanation = explainer([document], max_evals=num_samples)[0]
        return explanation


class SHAPLayoutExplainer(BaseShapExplainer):
    """Explain importance of the spatial (layout) modality.

    This explainer treats each bounding box as a feature and measures the
    influence of masking that box (and optionally the corresponding word).
    Perturbations are provided to SHAP as binary masks instead of strings.
    A sentinel tokenizer is used to break space‑delimited strings into tokens.
    """

    def __init__(
        self,
        model: torch.nn.Module,
        encode_fn: Callable[[List[DocSample], torch.device], dict],
        *,
        mask_token: str = "[UNK]",
        device: Optional[str] = None,
        algorithm: str = "partition",
        batch_size: int = 16,
        use_logit: bool = True,
    ) -> None:
        super().__init__(model, encode_fn, algorithm=algorithm, device=device, use_logit=use_logit)
        self.mask_token = mask_token
        self.batch_size = batch_size

    def _batched_predict(self, samples: List[DocSample]) -> np.ndarray:
        outputs: List[np.ndarray] = []
        for i in range(0, len(samples), self.batch_size):
            outputs.append(self._predict(samples[i : i + self.batch_size]))
        return np.vstack(outputs)

    def _make_predict_fn(self, sample: DocSample) -> Callable:
        """Return a function mapping binary masks to model outputs.

        Each binary mask has shape ``(n_tokens,)`` and indicates which
        features (bounding boxes) are kept (1) or masked (0).  When a box is
        masked the corresponding word is also replaced with the ``mask_token``
        and the box is expanded to cover the entire page.
        """
        width, height = sample.image.size
        full_box = [0, 0, width, height]

        def fn(masks: np.ndarray) -> np.ndarray:
            perturbed_samples: List[DocSample] = []
            for mask in masks:
                words: List[str] = []
                boxes: List = []
                for w, b, keep in zip(sample.words, sample.bboxes, mask):
                    if keep:
                        words.append(w)
                        boxes.append(b)
                    else:
                        words.append(self.mask_token)
                        boxes.append(full_box)
                perturbed_samples.append(
                    DocSample(
                        image=sample.image,
                        words=words,
                        bboxes=boxes,
                        ner_tags=sample.ner_tags,
                        label=sample.label,
                    )
                )
            return self._batched_predict(perturbed_samples)

        return fn

    def explain(self, sample: DocSample, *, num_samples: int = 2000) -> shap.Explanation:
        predict_fn = self._make_predict_fn(sample)
        link_fn = shap.links.logit if self.use_logit else shap.links.identity
        # Use a simple whitespace tokenizer for layout: each token corresponds to a
        # word in the document.  The ``masker`` will accept binary masks as
        # perturbations when called by SHAP.  We set ``output_type='string'``
        # because the predict function expects strings.
        masker = shap.maskers.Text(
            tokenizer=sentinel_tokenizer,
            mask_token=self.mask_token,
            collapse_mask_token="auto",
        )
        explainer = shap.Explainer(
            predict_fn,
            masker=masker,
            link=link_fn,
            algorithm=self.algorithm,
            output_names=self.class_names or None,
        )
        doc = " ".join(sample.words)
        explanation = explainer([doc], max_evals=num_samples)[0]
        return explanation


class SHAPVisionExplainer(BaseShapExplainer):
    """Explain importance of the visual modality (image) in VRDU models.

    This explainer masks portions of the input image using SHAP's image masker
    while keeping the associated text and bounding boxes fixed.  It supports
    both colour and greyscale images and can optionally restrict the output
    tensor to a single class for efficiency.
    """

    def __init__(
        self,
        model: torch.nn.Module,
        encode_fn: Callable[[List[DocSample], torch.device], dict],
        *,
        label: Optional[int] = None,
        mask_value: str | int | float | np.ndarray = "inpaint_telea",
        device: Optional[str] = None,
        algorithm: str = "partition",
        batch_size: int = 32,
        use_logit: bool = True,
    ) -> None:
        super().__init__(model, encode_fn, algorithm=algorithm, device=device, use_logit=use_logit)
        self.label = label
        self.mask_value = mask_value
        self.batch_size = batch_size

    def _batched_predict(self, samples: List[DocSample]) -> np.ndarray:
        outputs: List[np.ndarray] = []
        for i in range(0, len(samples), self.batch_size):
            outputs.append(self._predict(samples[i : i + self.batch_size]))
        return np.vstack(outputs)

    def _make_predict_fn(self, template: DocSample) -> Callable:
        """Return a function mapping arrays of images to model outputs.

        The returned callable takes a batch of numpy arrays of shape
        ``(H, W, C)`` (where ``C`` is 1 or 3) produced by SHAP.  Each array is
        wrapped into a ``DocSample`` with the original words and boxes from
        ``template``.  If ``label`` is set then the output probabilities (or
        logits) are reduced to a single column corresponding to that class.
        """

        def fn(images: np.ndarray) -> np.ndarray:
            perturbed_samples: List[DocSample] = []
            for img_arr in images:
                # Ensure dtype is uint8 for PIL
                if img_arr.dtype != np.uint8:
                    img_arr = img_arr.astype(np.uint8)
                img = Image.fromarray(img_arr)
                perturbed_samples.append(
                    DocSample(
                        image=img,
                        words=template.words,
                        bboxes=template.bboxes,
                        ner_tags=template.ner_tags,
                        label=template.label,
                    )
                )
            outputs = self._batched_predict(perturbed_samples)
            if self.label is not None:
                # Only keep the specified class; ensure the output is 2D
                outputs = outputs[:, self.label : self.label + 1]
            return outputs

        return fn

    def explain(
        self,
        sample: DocSample,
        *,
        num_samples: int = 8000,
        max_evals: Optional[int] = None,
        max_batch: int = 64,
    ) -> shap.Explanation:
        # Convert the image to a numpy array.  If greyscale (2D) add a channel.
        img_np = np.asarray(sample.image)
        if img_np.ndim == 2:
            img_np = img_np[..., None]
        # Create the SHAP image masker.  ``mask_value`` can be a string such as
        # "inpaint_telea" (uses OpenCV to inpaint), a scalar, or an array
        # matching the image shape.  See ``shap.maskers.Image`` for details.
        masker = shap.maskers.Image(self.mask_value, shape=img_np.shape)
        link_fn = shap.links.logit if self.use_logit else shap.links.identity
        explainer = shap.Explainer(
            self._make_predict_fn(sample),
            masker=masker,
            link=link_fn,
            algorithm=self.algorithm,
            output_names=self.class_names or None,
        )
        # Determine the number of evaluations (default to num_samples)
        max_evals = max_evals or num_samples
        explanation = explainer([img_np], max_evals=max_evals, batch_size=max_batch)[0]
        return explanation