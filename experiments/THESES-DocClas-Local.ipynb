{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Interpretability techniques for single samples from RVL-CDIP subset - LayoutLMVV3",
   "id": "db13c83d6d7e292d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GITHUB",
   "id": "41eb4c8f6d19cf87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!git clone https://github.com/adamserag1/Interpretability-for-VRDU-models.git",
   "id": "ae43f3703118361d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!git pull https://github.com/adamserag1/Interpretability-for-VRDU-models.git",
   "id": "35bff3e3c79c9ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%cd /content/Interpretability-for-VRDU-models",
   "id": "c2e68235ed6124a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install -r requirements.txt",
   "id": "22e445cf0b012bea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install -U datasets",
   "id": "51fcb68e9e4197b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Libraries",
   "id": "a6c2d8554347c001"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#code\n",
    "from datasets import load_from_disk\n",
    "from transformers import LayoutLMv3ForSequenceClassification, AutoProcessor, BrosModel, AutoTokenizer, BrosPreTrainedModel, AutoConfig\n",
    "import sys\n",
    "import importlib\n",
    "def reload_modules():\n",
    "    for module in list(sys.modules.keys()):\n",
    "        if module.startswith('vrdu_utils') or module.startswith('Explain') or module.startswith('lime') or module.startswith('Eval'):\n",
    "            print(f\"Reloading module: {module}\")\n",
    "            importlib.reload(sys.modules[module])\n",
    "\n",
    "reload_modules()\n",
    "\n",
    "from vrdu_utils.encoders import *\n",
    "from Explain.lime import *\n",
    "from vrdu_utils.utils import *\n",
    "import torch\n",
    "from Eval.eval_suite import *\n",
    "from Eval.evaluation import *\n",
    "# from Eval.fidelity import *\n",
    "from Explain.shap import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import warnings\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=FutureWarning,\n",
    "    module=\"transformers.modeling_utils\",   # the module that emits the msg\n",
    ")\n",
    "hf_logging.set_verbosity_error()\n"
   ],
   "id": "9ca518ee4c41a2a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data + Model Setup",
   "id": "d8159ebf28903bbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "!cp -r /content/drive/MyDrive/THESIS/rvl_cdip_financial_subset /content"
   ],
   "id": "805dabdad88928c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rvl = load_from_disk('/content/rvl_cdip_financial_subset')\n",
    "dataset_split = rvl.train_test_split(test_size=0.2, seed=42)\n",
    "val = dataset_split[\"test\"]\n",
    "val_ds = DocSampleDataset(val)\n",
    "\n",
    "AGREE = val_ds[846]\n",
    "CLASH = val_ds[282]"
   ],
   "id": "c36b5764c00215d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "AGREE[0].label",
   "id": "5d0224bde952ce49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "CLASH[0].image",
   "id": "e9ae0779fd7e22d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### BROSforDocumentClassifcation classifier head\n",
   "id": "a208a32d7368fe87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch import nn\n",
    "class BrosForDocumentClassification(BrosPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bros = BrosModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        bbox=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        outputs = self.bros(\n",
    "            input_ids=input_ids,\n",
    "            bbox=bbox,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits,\n",
    "        }"
   ],
   "id": "f2ef611b158f8955"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model config",
   "id": "afa32d8e533fa045"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "LLMV3 = LayoutLMv3ForSequenceClassification.from_pretrained(\"adamadam111/layoutlmv3-docclass-finetuned-frz\",\n",
    "                                                            num_labels=5,\n",
    "                                                            id2label={0: \"form\", 1: \"invoice\", 2: \"budget\", 3: \"file folder\", 4: \"questionnaire\"},\n",
    "                                                            label2id={\"form\": 0, \"invoice\": 1, \"budget\": 2, \"file folder\": 3, \"questionnaire\": 4})\n",
    "LLMV3_proc = AutoProcessor.from_pretrained(\"adamadam111/layoutlmv3-docclass-finetuned-frz\", apply_ocr=False)\n",
    "\n",
    "LLMV3.to(device)\n",
    "\n",
    "bros_config = AutoConfig.from_pretrained(\n",
    "    \"adamadam111/bros-docclass-finetuned-frz\",\n",
    "    num_labels=5,\n",
    "    id2label={0: \"form\", 1: \"invoice\", 2: \"budget\", 3: \"file folder\", 4: \"questionnaire\"},\n",
    "    label2id={\"form\": 0, \"invoice\": 1, \"budget\": 2, \"file folder\": 3, \"questionnaire\": 4}\n",
    ")\n",
    "\n",
    "BROS = BrosForDocumentClassification.from_pretrained(\n",
    "    \"adamadam111/bros-docclass-finetuned-frz\",\n",
    "    config=bros_config\n",
    ")\n",
    "BROS_t = AutoTokenizer.from_pretrained(\"adamadam111/bros-docclass-finetuned-frz\",do_lower_case=True)\n"
   ],
   "id": "52bea5b0cfe50ac4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Interpreting the 'CLASH' sample",
   "id": "da8e7535ee80d595"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "LLMV3_encode = make_layoutlmv3_encoder(LLMV3_proc)\n",
    "BROS_encode = make_bros_encoder(BROS_t)\n",
    "\n",
    "pred_fn_llmv3 = FidelityEvaluator(LLMV3, LLMV3_encode, mask_token=LLMV3_proc.tokenizer.mask_token)._get_prediction_function(4)\n",
    "pred_fn_llmv3 = FidelityEvaluator(BROS, BROS_encode, mask_token=BROS_t.mask_token)._get_prediction_function(0)\n"
   ],
   "id": "2c5a09920f973dc3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Text Modality\n",
   "id": "8030e0b232addc23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7766786be1a38bc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(CLASH[0].words)",
   "id": "a27942d2d9ca4b90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!git pull https://github.com/adamserag1/Interpretability-for-VRDU-models.git",
   "id": "551403c4e78f6fdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from Explain.lime import *\n",
    "from Explain.shap import *"
   ],
   "id": "e0470026346b699c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "text_explainers = {\n",
    "    'BROS lime' : LimeTextExplainer(BROS, BROS_encode, mask_token=BROS_t.mask_token, kernel_width_factor = 0.75, labels=[4,0]),\n",
    "    'LLMV3 lime' : LimeTextExplainer(LLMV3, LLMV3_encode, mask_token = LLMV3_proc.tokenizer.mask_token, kernel_width_factor = 0.75, labels = [4,0]),\n",
    "    'BROS shap' : SHAPTextExplainer(BROS, BROS_encode, BROS_t, mask_token=BROS_t.mask_token, device=device),\n",
    "    'LLMV3 shap' : SHAPTextExplainer(LLMV3, LLMV3_encode, LLMV3_proc.tokenizer,mask_token=LLMV3_proc.tokenizer.mask_token, device=device)\n",
    "}"
   ],
   "id": "c5ea46f1da30f58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# RUN FOR\n",
    "shap_text_explainations = {key : explainer.explain(CLASH[0], nsamples = 4000) for key, explainer in text_explainers.items() if 'shap' in key}\n",
    "lime_text_explainations = {key : explainer.explain(CLASH[0], num_samples = 4000) for key, explainer in text_explainers.items() if 'lime' in key}"
   ],
   "id": "2c0c9f7d337e1bd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2947cea4e653e9bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "bros_pred = text_explainers['BROS lime']._predict([CLASH[0]])\n",
    "llmv3_pred = text_explainers['LLMV3 lime']._predict([CLASH[0]]).flatten()\n",
    "print(llmv3_pred.argmax())\n",
    "print(bros_pred, llmv3_pred)\n",
    "CLASH[0].label"
   ],
   "id": "7f5ec28d682423af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Pertaining to label 4 (correct, Questionnaire) (idx 0 shap)\n",
    "text_explanations_hms = {}\n",
    "text_explanations_weights = {}\n",
    "for key, explanation in shap_text_explainations.items():\n",
    "  weights = {tok : float(val) for tok, val in zip(CLASH[0].words, shap_text_explainations[key].values[:,:,4].flatten())}\n",
    "  text_explanations_weights.update({key : weights})\n",
    "  text_explanations_hms.update({key : draw_lime_token_heatmap(image = CLASH[0].image, words = CLASH[0].words, boxes = CLASH[0].bboxes, weights=weights, alpha=0.5) })\n",
    "\n",
    "for key, explanation in lime_text_explainations.items():\n",
    "  weights = dict(lime_text_explainations[key].as_list(label=4))\n",
    "  clean_weights = {key.replace('=1', ''): value for key, value in weights.items()}\n",
    "  text_explanations_weights.update({key : clean_weights})\n",
    "  text_explanations_hms.update({key : draw_lime_token_heatmap(image = CLASH[0].image, words = CLASH[0].words, boxes = CLASH[0].bboxes, weights=clean_weights, alpha=0.5) })\n",
    "\n",
    "# LLMV3 Fidelity\n",
    "# pred_fn_llmv3_s = FidelityEvaluator(LLMV3, LLMV3_encode, mask_token=LLMV3_proc.tokenizer.mask_token)._get_prediction_function(4)\n",
    "# pred_fn_llmv3_l = FidelityEvaluator(LLMV3, LLMV3_encode, mask_token='|~|')._get_prediction_function(4)\n",
    "# comp_s = calculate_comprehensiveness(pred_fn_llmv3_s, CLASH[0], text_explanations_weights['LLMV3 shap'], mask_token='|~|',  top_k=5)\n",
    "# comp_l = calculate_comprehensiveness(pred_fn_llmv3_l, CLASH[0], text_explanations_weights['LLMV3 lime'], mask_token=LLMV3_proc.tokenizer.mask_token,  top_k=5)\n",
    "# suf_s = calculate_sufficiency(pred_fn_llmv3_s, CLASH[0], text_explanations_weights['LLMV3 shap'], mask_token='|~|',  top_k=5)\n",
    "# suf_l = calculate_sufficiency(pred_fn_llmv3_l, CLASH[0], text_explanations_weights['LLMV3 lime'], mask_token=LLMV3_proc.tokenizer.mask_token,  top_k=5)\n",
    "\n",
    "# display_image_grid([text_explanations_hms['LLMV3 lime'], text_explanations_hms['LLMV3 shap']],\n",
    "#                    [f'LLMV3 - Lime w.r.t text\\nComprehensiveness = {comp_l:.3g}, Sufficiency = {suf_l:.3g}', f'LLMV3 - SHAP w.r.t text\\nComprehensiveness = {comp_s:.3g}, Sufficiency = {suf_s:.3g}'],\n",
    "#                     (1,2),\n",
    "#                    main_title='Layout LMV3 Correct Prediction w.r.t Text\\n \"Questionnaire\"')\n",
    "\n",
    "# text_explanations_hms['LLMV3 shap']\n",
    "text_explanations_hms['LLMV3 lime']"
   ],
   "id": "37f32f23498c6753"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "text_explanations_hms = {}\n",
    "text_explanations_weights = {}\n",
    "for key, explanation in shap_text_explainations.items():\n",
    "  weights = {tok : float(val) for tok, val in zip(CLASH[0].words, shap_text_explainations[key].values[:,:,0].flatten())}\n",
    "  text_explanations_weights.update({key : weights})\n",
    "  text_explanations_hms.update({key : draw_lime_token_heatmap(image = CLASH[0].image, words = CLASH[0].words, boxes = CLASH[0].bboxes, weights=weights, alpha=0.5) })\n",
    "\n",
    "for key, explanation in lime_text_explainations.items():\n",
    "  weights = dict(lime_text_explainations[key].as_list(label=0))\n",
    "  clean_weights = {key.replace('=1', ''): value for key, value in weights.items()}\n",
    "  text_explanations_weights.update({key : clean_weights})\n",
    "  text_explanations_hms.update({key : draw_lime_token_heatmap(image = CLASH[0].image, words = CLASH[0].words, boxes = CLASH[0].bboxes, weights=clean_weights, alpha=0.5) })\n",
    "\n",
    "# LLMV3 Fidelity\n",
    "pred_fn_bros_s = FidelityEvaluator(BROS, BROS_encode, mask_token=BROS_t.mask_token)._get_prediction_function(0)\n",
    "pred_fn_bros_l = FidelityEvaluator(BROS, BROS_encode, mask_token='|~|')._get_prediction_function(0)\n",
    "comp_s = calculate_comprehensiveness(pred_fn_bros_s, CLASH[0], text_explanations_weights['BROS shap'], mask_token='|~|',  top_k=5)\n",
    "comp_l = calculate_comprehensiveness(pred_fn_bros_l, CLASH[0], text_explanations_weights['BROS lime'], mask_token=BROS_t.mask_token, top_k=5)\n",
    "suf_s = calculate_sufficiency(pred_fn_bros_s, CLASH[0], text_explanations_weights['BROS shap'], mask_token='|~|', top_k=5)\n",
    "suf_l = calculate_sufficiency(pred_fn_bros_l, CLASH[0], text_explanations_weights['BROS lime'], mask_token=BROS_t.mask_token, top_k=5)\n",
    "\n",
    "# display_image_grid([text_explanations_hms['BROS lime'], text_explanations_hms['BROS shap']],\n",
    "#                    [f'BROS - Lime\\nComprehensiveness = {comp_l:.3g}, Sufficiency = {suf_l:.3g}', f'BROS - SHAP\\nComprehensiveness = {comp_s:.3g}, Sufficiency = {suf_s:.3g}'],\n",
    "#                     (1,2),\n",
    "#                    main_title='BROS inorrect Prediction w.r.t Text\\n \"Form\"')\n",
    "\n",
    "# text_explanations_hms['BROS lime']\n",
    "text_explanations_hms['BROS shap']"
   ],
   "id": "604ec27b665f72d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "text_explanations_hms = {}\n",
    "text_explanations_weights = {}\n",
    "for key, explanation in shap_text_explainations.items():\n",
    "  weights = {tok : float(val) for tok, val in zip(CLASH[0].words, shap_text_explainations[key].values[:, 0])}\n",
    "  text_explanations_weights.update({key : weights})\n",
    "  text_explanations_hms.update({key : draw_lime_token_heatmap(image = CLASH[0].image, words = CLASH[0].words, boxes = CLASH[0].bboxes, weights=weights, alpha=0.5) })\n",
    "\n",
    "for key, explanation in lime_text_explainations.items():\n",
    "  weights = dict(lime_text_explainations[key].as_list(label=0))\n",
    "  clean_weights = {key.replace('=1', ''): value for key, value in weights.items()}\n",
    "  text_explanations_weights.update({key : clean_weights})\n",
    "  text_explanations_hms.update({key : draw_lime_token_heatmap(image = CLASH[0].image, words = CLASH[0].words, boxes = CLASH[0].bboxes, weights=clean_weights, alpha=0.5) })"
   ],
   "id": "171918120fdcfc38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## Pertaining to label 0 (incorrect, Form)",
   "id": "d262e2953ab75d34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": " print(shap_text_explainations['BROS shap'].values[:,:,4].flatten())",
   "id": "4bb8b19c41a8794e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(lime_text_explainations['BROS lime'])",
   "id": "62c69395f18d5921"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print( [dict(lime_text_explainations['BROS lime'].as_list(label=0))])",
   "id": "160f30c7082b9bae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "CLASH[0].label",
   "id": "c1c5eedf874ef5f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# FIDELTI\n",
    "from Eval.evaluation import evaluate_sample as eval\n",
    "# from Eval.eval_suite_updated import calculate_sufficiency as calculate_sufficiency\n",
    "def predict_fn_bros(sample):\n",
    "    enc = BROS_encode([sample], device)\n",
    "    pred = BROS(**enc)\n",
    "    logits = pred['logits']\n",
    "    probs  = torch.softmax(logits, -1)[0]\n",
    "    return probs[0].item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_fn_llmv3(sample):\n",
    "    enc = LLMV3_encode([sample], device)\n",
    "    logits =  LLMV3(**enc).logits\n",
    "    probs  = torch.softmax(logits, -1)[0]\n",
    "    return probs[4].item()\n",
    "\n",
    "\n",
    "# class_ids_dict = {\n",
    "#     \"BROS lime\"  : 0,   # probability of class-0 for BROS\n",
    "#     \"LLMV3 lime\" : 4,   # probability of class-4 for LayoutLMv3\n",
    "#     \"BROS shap\"  : 0,\n",
    "#     \"LLMV3 shap\" : 4,\n",
    "# }\n",
    "\n",
    "# models_dict = {\n",
    "#     'BROS lime' : BROS,\n",
    "#     'LLMV3 lime' : LLMV3,\n",
    "#     'BROS shap' : BROS,\n",
    "#     'LLMV3 shap' : LLMV3\n",
    "# }\n",
    "\n",
    "# encoders_dict = {\n",
    "#     'BROS lime' : BROS_encode,\n",
    "#     'LLMV3 lime' : LLMV3_encode,\n",
    "#     'BROS shap' : BROS_encode,\n",
    "#     'LLMV3 shap' : LLMV3_encode\n",
    "# }\n",
    "\n",
    "# mask_tokens_dict = {\n",
    "#     'BROS lime' : BROS_t.mask_token,\n",
    "#     'LLMV3 lime' : LLMV3_proc.tokenizer.mask_token,\n",
    "#     'BROS shap' : BROS_t.mask_token,\n",
    "#     'LLMV3 shap' : LLMV3_proc.tokenizer.mask_token\n",
    "# }\n",
    "# text_explanations_dict = {\n",
    "#     'BROS lime' : [dict(lime_text_explainations['BROS lime'].as_list(label=0))],\n",
    "#     'LLMV3 lime' : [dict(lime_text_explainations['LLMV3 lime'].as_list(label=4))],\n",
    "#     'BROS shap' : [{tok : float(val) for tok, val in zip(CLASH[0].words, shap_text_explainations['BROS shap'].values[:,:,0].flatten())}],\n",
    "#     'LLMV3 shap' : [{tok : float(val) for tok, val in zip(CLASH[0].words, shap_text_explainations['LLMV3 shap'].values[:,:,4].flatten())}]\n",
    "# }\n",
    "\n",
    "# aopc = compute_aopc_curves([CLASH[0]], text_explanations_dict, modality=\"text\", models_dict=models_dict, encoders_dict=encoders_dict, mask_tokens_dict=mask_tokens_dict, class_ids_dict = class_ids_dict, max_k=50)\n",
    "# plot_aopc_curves(aopc, modality=\"text\")\n",
    "# text_explanations_dict = shap_text_explainations\n",
    "# print(lime_text_explainations)\n",
    "# text_explanations_dict.update({k : d.as_list(label=0) for k, d in lime_text_explainations.items()})\n",
    "# print(text_explanations_dict)\n",
    "# print(len(shap_text_explainations))\n",
    "shap_text_fid = {}\n",
    "lime_text_fid = {}\n",
    "for key,exp in shap_text_explainations.items():\n",
    "  if 'LLMV3' in key:\n",
    "    weights = {tok : float(val) for tok, val in zip(CLASH[0].words, exp.values[:,:,4].flatten())}\n",
    "    shap_text_fid.update({key : eval(CLASH[0], weights, 'text', LLMV3, LLMV3_encode, top_k=5, device=device, mask_token=LLMV3_proc.tokenizer.mask_token, target_class_id=4)})\n",
    "\n",
    "  if 'BROS' in key:\n",
    "    weights = {tok : float(val) for tok, val in zip(CLASH[0].words, exp.values[:,:,0].flatten())}\n",
    "    shap_text_fid.update({key : eval(CLASH[0], weights, 'text', BROS, BROS_encode, top_k=5, device=device, mask_token=BROS_t.mask_token,  target_class_id=0)})\n",
    "\n",
    "\n",
    "for key,exp in lime_text_explainations.items():\n",
    "  if 'LLMV3' in key:\n",
    "    weights = dict(exp.as_list(label=0))\n",
    "    clean_weights = {key.replace('=1', ''): value for key, value in weights.items()}\n",
    "    shap_text_fid.update({key : eval(CLASH[0], clean_weights, 'text', LLMV3, LLMV3_encode, top_k=5, device=device, mask_token=LLMV3_proc.tokenizer.mask_token,  target_class_id=4)})\n",
    "\n",
    "  if 'BROS' in key:\n",
    "    weights = dict(exp.as_list(label=0))\n",
    "    clean_weights = {key.replace('=1', ''): value for key, value in weights.items()}\n",
    "    shap_text_fid.update({key : eval(CLASH[0], clean_weights, 'text', BROS, BROS_encode, top_k=5, device=device, mask_token=BROS_t.mask_token,  target_class_id=0)})\n",
    "\n",
    "print(lime_text_fid)\n",
    "print(shap_text_fid)"
   ],
   "id": "a192e46a94fe3698"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Layout",
   "id": "884b4f1af14bc03d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "layout_explainers = {\n",
    "    'BROS lime' : LimeLayoutExplainer(BROS, BROS_encode, mask_token=BROS_t.mask_token, kernel_width_factor = 0.75, labels=[4,0]),\n",
    "    'LLMV3 lime' : LimeLayoutExplainer(LLMV3, LLMV3_encode, mask_token = LLMV3_proc.tokenizer.mask_token, kernel_width_factor = 0.75, labels = [4,0]),\n",
    "    'BROS shap' : SHAPLayoutExplainer(BROS, BROS_encode, device=device),\n",
    "    'LLMV3 shap' : SHAPLayoutExplainer(LLMV3, LLMV3_encode, device=device)\n",
    "}"
   ],
   "id": "136499cb226310f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "shap_layout_explainations = {key : explainer.explain(CLASH[0], nsamples = 4000) for key, explainer in layout_explainers.items() if 'shap' in key}\n",
    "lime_layout_explainations = {key : explainer.explain(CLASH[0], num_samples = 4000) for key, explainer in layout_explainers.items() if 'lime' in key}"
   ],
   "id": "755d1a23ed2c7299"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Pertaining to label 4 (correct, Questionnaire) (idx 0 shap)\n",
    "layout_explanations_hms = {}\n",
    "layout_explanations_weights = {}\n",
    "for key, explanation in shap_layout_explainations.items():\n",
    "  weights = {tok : float(val) for tok, val in zip(CLASH[0].words, shap_layout_explainations[key].values[:,:,4].flatten())}\n",
    "  layout_explanations_weights.update({key : weights})\n",
    "  layout_explanations_hms.update({key : draw_lime_token_heatmap(image = CLASH[0].image, words = CLASH[0].words, boxes = CLASH[0].bboxes, weights=weights, alpha=0.5) })\n",
    "\n",
    "for key, explanation in lime_layout_explainations.items():\n",
    "  weights = dict(lime_layout_explainations[key].as_list(label=4))\n",
    "  clean_weights = {key.replace('=1', ''): value for key, value in weights.items()}\n",
    "  layout_explanations_weights.update({key : clean_weights})\n",
    "  layout_explanations_hms.update({key : draw_lime_token_heatmap(image = CLASH[0].image, words = CLASH[0].words, boxes = CLASH[0].bboxes, weights=clean_weights, alpha=0.5) })\n",
    "\n",
    "# LLMV3 Fidelity\n",
    "pred_fn_llmv3_s = FidelityEvaluator(LLMV3, LLMV3_encode, mask_token=LLMV3_proc.tokenizer.mask_token)._get_prediction_function(4)\n",
    "pred_fn_llmv3_l = FidelityEvaluator(LLMV3, LLMV3_encode, mask_token='|~|')._get_prediction_function(4)\n",
    "comp_s = calculate_comprehensiveness(pred_fn_llmv3_s, CLASH[0], layout_explanations_weights['LLMV3 shap'], mask_token=LLMV3_proc.tokenizer.mask_token,  top_k=10, modality='layout')\n",
    "comp_l = calculate_comprehensiveness(pred_fn_llmv3_l, CLASH[0], layout_explanations_weights['LLMV3 lime'], mask_token=LLMV3_proc.tokenizer.mask_token,  top_k=10,modality='layout')\n",
    "suf_s = calculate_sufficiency(pred_fn_llmv3_s, CLASH[0], layout_explanations_weights['LLMV3 shap'], LLMV3_proc.tokenizer.mask_token,  top_k=5,modality='layout')\n",
    "suf_l = calculate_sufficiency(pred_fn_llmv3_l, CLASH[0], layout_explanations_weights['LLMV3 lime'], mask_token=LLMV3_proc.tokenizer.mask_token,  top_k=5,modality='layout')\n",
    "\n",
    "# display_image_grid([layout_explanations_hms['LLMV3 lime'], layout_explanations_hms['LLMV3 shap']],\n",
    "#                    [f'LLMV3 - Lime w.r.t layout\\nComprehensiveness = {comp_l:.3g}, Sufficiency = {suf_l:.3g}', f'LLMV3 - SHAP w.r.t layout\\nComprehensiveness = {comp_s:.3g}, Sufficiency = {suf_s:.3g}'],\n",
    "#                     (1,2),\n",
    "#                    main_title='Layout LMV3 Correct Prediction w.r.t Layout\\n \"Questionnaire\"')\n",
    "\n",
    "# layout_explanations_hms['LLMV3 lime']\n",
    "layout_explanations_hms['LLMV3 shap']"
   ],
   "id": "1584edf896b4184a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Pertaining to label 4 (correct, Questionnaire) (idx 0 shap)\n",
    "layout_explanations_hms = {}\n",
    "layout_explanations_weights = {}\n",
    "for key, explanation in shap_layout_explainations.items():\n",
    "  weights = {tok : float(val) for tok, val in zip(CLASH[0].words, shap_layout_explainations[key].values[:,:,0].flatten())}\n",
    "  layout_explanations_weights.update({key : weights})\n",
    "  layout_explanations_hms.update({key : draw_lime_token_heatmap(image = CLASH[0].image, words = CLASH[0].words, boxes = CLASH[0].bboxes, weights=weights, alpha=0.5) })\n",
    "\n",
    "for key, explanation in lime_layout_explainations.items():\n",
    "  weights = dict(lime_layout_explainations[key].as_list(label=0))\n",
    "  clean_weights = {key.replace('=1', ''): value for key, value in weights.items()}\n",
    "  layout_explanations_weights.update({key : clean_weights})\n",
    "  layout_explanations_hms.update({key : draw_lime_token_heatmap(image = CLASH[0].image, words = CLASH[0].words, boxes = CLASH[0].bboxes, weights=clean_weights, alpha=0.5) })\n",
    "\n",
    "# LLMV3 Fidelity\n",
    "pred_fn_bros_s = FidelityEvaluator(BROS, BROS_encode, mask_token=BROS_t.mask_token)._get_prediction_function(0)\n",
    "pred_fn_bros_l = FidelityEvaluator(BROS, BROS_encode, mask_token=BROS_t.mask_token)._get_prediction_function(0)\n",
    "comp_s = calculate_comprehensiveness(pred_fn_bros_s, CLASH[0], layout_explanations_weights['BROS shap'], mask_token=BROS_t.mask_token,  top_k=10, modality='layout')\n",
    "comp_l = calculate_comprehensiveness(pred_fn_bros_l, CLASH[0], layout_explanations_weights['BROS lime'], mask_token=BROS_t.mask_token,  top_k=5,modality='layout')\n",
    "suf_s = calculate_sufficiency(pred_fn_bros_s, CLASH[0], layout_explanations_weights['BROS shap'], mask_token=BROS_t.mask_token,  top_k=5,modality='layout')\n",
    "suf_l = calculate_sufficiency(pred_fn_bros_l, CLASH[0], layout_explanations_weights['BROS lime'], mask_token=BROS_t.mask_token,  top_k=5,modality='layout')\n",
    "\n",
    "# display_image_grid([layout_explanations_hms['BROS lime'], layout_explanations_hms['BROS shap']],\n",
    "#                    [f'LLMV3 - Lime w.r.t layout\\nComprehensiveness = {comp_l:.3g}, Sufficiency = {suf_l:.3g}', f'LLMV3 - SHAP w.r.t layout\\nComprehensiveness = {comp_s:.3g}, Sufficiency = {suf_s:.3g}'],\n",
    "#                     (1,2),\n",
    "#                    main_title='Layout LMV3 Correct Prediction w.r.t Layout\\n \"Questionnaire\"')\n",
    "\n",
    "layout_explanations_hms['BROS shap']\n",
    "# layout_explanations_hms['BROS shap']"
   ],
   "id": "7590a6091ad3c7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(shap_layout_explainations.keys())\n",
    "print(lime_layout_explainations.keys())\n"
   ],
   "id": "fe9fb86eb1d76a73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "shap_layout_fid = {}\n",
    "lime_layout_fid = {}\n",
    "\n",
    "for key, exp in shap_layout_explainations.items():\n",
    "    if \"LLMV3\" in key:                                  # class-4 explanation\n",
    "        scores   = exp.values[:, :, 4].flatten()        # (#tokens,)\n",
    "        weights  = {tuple(box): float(s)\n",
    "                    for box, s in zip(CLASH[0].bboxes, scores)}\n",
    "        shap_layout_fid[key] = eval(CLASH[0], weights, \"layout\",\n",
    "                                    LLMV3, LLMV3_encode,\n",
    "                                    top_k=5, device=device,\n",
    "                                    mask_token=LLMV3_proc.tokenizer.mask_token,\n",
    "                                    target_class_id=4)\n",
    "\n",
    "    if \"BROS\" in key:                                   # class-0 explanation\n",
    "        scores   = exp.values[:, :, 0].flatten()\n",
    "        weights  = {tuple(box): float(s)\n",
    "                    for box, s in zip(CLASH[0].bboxes, scores)}\n",
    "        shap_layout_fid[key] = eval(CLASH[0], weights, \"layout\",\n",
    "                                    BROS, BROS_encode,\n",
    "                                    top_k=5, device=device,\n",
    "                                    mask_token=BROS_t.mask_token,\n",
    "                                    target_class_id=0)\n",
    "\n",
    "\n",
    "for key, exp in lime_layout_explainations.items():\n",
    "    if \"LLMV3\" in key:\n",
    "        token_w = dict(exp.as_map()[4])                 # class-4 map\n",
    "        bbox_w  = {tuple(CLASH[0].bboxes[i]): w\n",
    "                   for i, w in token_w.items()\n",
    "                   if i < len(CLASH[0].bboxes)}\n",
    "        lime_layout_fid[key] = eval(CLASH[0], bbox_w, \"layout\",\n",
    "                                     LLMV3, LLMV3_encode,\n",
    "                                     top_k=5, device=device,\n",
    "                                     mask_token=LLMV3_proc.tokenizer.mask_token,\n",
    "                                     target_class_id=4)\n",
    "\n",
    "    if \"BROS\" in key:\n",
    "        token_w = dict(exp.as_map()[0])\n",
    "        bbox_w  = {tuple(CLASH[0].bboxes[i]): w\n",
    "                   for i, w in token_w.items()\n",
    "                   if i < len(CLASH[0].bboxes)}\n",
    "        lime_layout_fid[key] = eval(CLASH[0], bbox_w, \"layout\",\n",
    "                                     BROS, BROS_encode,\n",
    "                                     top_k=5, device=device,\n",
    "                                     mask_token=BROS_t.mask_token,\n",
    "                                     target_class_id=0)\n",
    "print(lime_layout_fid)\n",
    "print(shap_layout_fid)"
   ],
   "id": "aa5ef0fcfc91ac96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Vision",
   "id": "7f9759618b6688bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vision_explainers = {\n",
    "    'LLMV3 lime' : LimeVisionExplainer(LLMV3, LLMV3_encode, label = [4], device=device),\n",
    "    'LLMV3 shap' : SHAPVisionExplainer(LLMV3, LLMV3_encode, device=device, class_idx=4, mask_value='blur(64,64)')\n",
    "}"
   ],
   "id": "5d9f72dcf548855b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(vision_explanations['LLMV3 shap'].values.shape)",
   "id": "d40d3bfd85492abc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vision_explanations = {key : explainer.explain(CLASH[0], num_samples = 1000) for key, explainer in vision_explainers.items() if 'lime' in key}\n",
    "vision_explanations.update({key : explainer.explain(CLASH[0], nsamples = 1000) for key, explainer in vision_explainers.items() if 'shap' in key})\n",
    "# temp = vision_explainers['LLMV3 shap'].explain(CLASH[0], nsamples = 1000, max_batch = 32)"
   ],
   "id": "4437c56f79959b4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4e979330b5f4f2d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(vision_explanations['LLMV3 shap'])",
   "id": "83943da109b76e5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from Eval.evaluation import evaluate_sample\n",
    "from skimage.segmentation import slic\n",
    "from PIL import Image\n",
    "\n",
    "# Lime\n",
    "lime_segs = vision_explanations['LLMV3 lime'].segments\n",
    "seg_weights = {int(sid): float(w) for sid, w in vision_explanations['LLMV3 lime'].local_exp[4]}\n",
    "\n",
    "metrics_lime = evaluate_sample(CLASH[0], seg_weights, 'vision',\n",
    "                               LLMV3, LLMV3_encode,\n",
    "                               top_k = 10,\n",
    "                               device = device,\n",
    "                               target_class_id = 4,\n",
    "                               segments = lime_segs)\n",
    "\n",
    "print(\"lime:\", metrics_lime)\n",
    "\n",
    "# SHAP\n",
    "\n",
    "shap_vals = vision_explanations['LLMV3 shap'].values   # (H,W,3) float\n",
    "abs_map   = np.abs(shap_vals).sum(-1)                    # (H,W)\n",
    "\n",
    "segments_S = lime_segs\n",
    "\n",
    "weights_S = {int(sid): float(abs_map[segments_S == sid].mean())\n",
    "             for sid in np.unique(segments_S)}\n",
    "\n",
    "metrics_shap = evaluate_sample(CLASH[0], weights_S, \"vision\",\n",
    "                               LLMV3, LLMV3_encode,\n",
    "                               top_k           = 10,\n",
    "                               device          = device,\n",
    "                               target_class_id = 4,\n",
    "                               segments        = segments_S)\n",
    "\n",
    "print(\"SHAP – vision:\", metrics_shap)\n"
   ],
   "id": "c5062bda75d75ef8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def shap_overlay(exp, alpha=0.6, cmap=\"bwr\"):\n",
    "    vals  = exp.values          # (H,W) or (H,W,3)\n",
    "    img   = exp.data.astype(np.uint8)   # (H,W,3)\n",
    "\n",
    "    if vals.ndim == 3:                   # SHAP gave per-channel values\n",
    "        vals = vals.mean(2)              # collapse to one heat-map\n",
    "\n",
    "    vmax  = np.abs(vals).max() + 1e-12\n",
    "    norm  = (vals + vmax) / (2 * vmax)   # 0…1\n",
    "    rgb   = plt.get_cmap(cmap)(norm)[..., :3] * 255  # (H,W,3) uint8\n",
    "    blend = (img*(1-alpha) + rgb*alpha).astype(np.uint8)\n",
    "    return Image.fromarray(blend)\n",
    "\n",
    "shap_overlay(vision_explanations['LLMV3 shap'], alpha=0.6)\n"
   ],
   "id": "903ae70ca3181d5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np, matplotlib.pyplot as plt, matplotlib.cm as cm\n",
    "from PIL import Image\n",
    "from skimage.segmentation import slic\n",
    "\n",
    "def lime_heatmap(expl, class_idx=4, alpha_max=0.6, cmap=\"bwr\"):\n",
    "    img     = expl.image.astype(float)\n",
    "    segs    = expl.segments\n",
    "    weights = dict(expl.local_exp[class_idx])\n",
    "    vmax    = max(abs(w) for w in weights.values()) + 1e-12\n",
    "\n",
    "    overlay = img.copy()\n",
    "    for seg_id, w in weights.items():\n",
    "        mask   = segs == seg_id\n",
    "        color  = np.array(cm.get_cmap(cmap)((w+vmax)/(2*vmax))[:3]) * 255\n",
    "        alpha  = alpha_max * abs(w) / vmax\n",
    "        overlay[mask] = overlay[mask]*(1-alpha) + color*alpha\n",
    "\n",
    "    return Image.fromarray(overlay.astype(np.uint8))\n",
    "\n",
    "expl  = vision_explanations['LLMV3 lime']\n",
    "hm    = lime_heatmap(expl, class_idx=4, alpha_max=0.7)\n",
    "hm"
   ],
   "id": "9a82d15d4431ac5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pred_f_l = vision_explainers['LLMV3 lime']._batched_predict\n",
    "pred_f_s = vision_explainers['LLMV3 shap']._batched_predict\n",
    "lime_drop, im1   = lime_comprehensiveness(pred_f_l, CLASH[0], vision_explanations['LLMV3 lime'],4, 2 )\n",
    "lime_keep, im2   = lime_sufficiency(pred_f_l, CLASH[0], vision_explanations['LLMV3 lime'],4,2)\n",
    "print(lime_drop, lime_keep)\n",
    "# SHAP\n",
    "shap_drop, im3   = shap_comprehensiveness(pred_f_l, CLASH[0], temp, 1)\n",
    "shap_keep, im4   = shap_sufficiency(pred_f_l, CLASH[0], temp,50)\n",
    "print(shap_drop, shap_keep)\n",
    "display_image_grid([im1,im2,im3,im4], titles=['ld', 'lk', 'sd', 'sk'], grid_size=(2,2),main_title='fidComp')"
   ],
   "id": "5ca74a140fce05e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Trouble importing from evaluation so done in notebook\n",
    "\n",
    "def _predict_prob(predict_fn, sample):\n",
    "    logits = predict_fn([sample])[0]\n",
    "    if logits.ndim == 0:\n",
    "        return 1 / (1 + np.exp(-logits))\n",
    "    cls = np.argmax(logits)\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    return probs[cls]\n",
    "\n",
    "\n",
    "def _mask_image(img, mask, hide=(255, 255, 255)):\n",
    "    out = img.copy()\n",
    "    out[mask] = hide\n",
    "    return out\n",
    "\n",
    "\n",
    "def _doc_from_img(img_np, t):\n",
    "    return DocSample(\n",
    "        image=Image.fromarray(img_np),\n",
    "        words=t.words,\n",
    "        bboxes=t.bboxes,\n",
    "        ner_tags=t.ner_tags,\n",
    "        label=t.label,\n",
    "    )\n",
    "\n",
    "\n",
    "def _topk_lime_segments(weights: dict, k: int):\n",
    "    # keep only positive weights\n",
    "    pos = {sid: w for sid, w in weights.items() if w > 0}\n",
    "    if not pos:\n",
    "        return []\n",
    "    return sorted(pos, key=pos.get, reverse=True)[:k]\n",
    "\n",
    "\n",
    "def lime_comprehensiveness(predict, sample, exp, class_idx, k=5):\n",
    "    w = dict(exp.local_exp[class_idx])\n",
    "    top = _topk_lime_segments(w, k)\n",
    "\n",
    "    seg = exp.segments\n",
    "    mask = np.isin(seg, top)\n",
    "\n",
    "    img = np.asarray(sample.image.convert(\"RGB\"))\n",
    "    masked = _mask_image(img, mask)\n",
    "    pert = _doc_from_img(masked, sample)\n",
    "\n",
    "    d = _predict_prob(predict, sample) - _predict_prob(predict, pert)\n",
    "    return d, Image.fromarray(masked)\n",
    "\n",
    "\n",
    "def lime_sufficiency(predict, sample, exp, class_idx, k=5):\n",
    "    w = dict(exp.local_exp[class_idx])\n",
    "    top = _topk_lime_segments(w, k)\n",
    "\n",
    "    seg = exp.segments\n",
    "    mask = ~np.isin(seg, top)\n",
    "\n",
    "    img = np.asarray(sample.image.convert(\"RGB\"))\n",
    "    masked = _mask_image(img, mask)\n",
    "    pert = _doc_from_img(masked, sample)\n",
    "\n",
    "    d = _predict_prob(predict, sample) - _predict_prob(predict, pert)\n",
    "    return d, Image.fromarray(masked)\n",
    "\n",
    "\n",
    "def _topk_shap_segments(vals, img_np, k,\n",
    "                        n_segments=150, compactness=10, sigma=1):\n",
    "    segs = slic(img_np, n_segments=n_segments,\n",
    "                compactness=compactness, sigma=sigma, start_label=0)\n",
    "    means = {sid: vals[segs == sid].mean() for sid in np.unique(segs)}\n",
    "    pos = {sid: m for sid, m in means.items() if m > 0}\n",
    "    if not pos:\n",
    "        return np.zeros_like(vals, bool)\n",
    "    top = sorted(pos, key=pos.get, reverse=True)[:k]\n",
    "    return np.isin(segs, top)\n",
    "\n",
    "\n",
    "def shap_comprehensiveness(predict, sample, exp, k=10, slic_kw=None):\n",
    "    vals = exp.values.mean(2) if exp.values.ndim == 3 else exp.values\n",
    "    img = np.asarray(sample.image.convert(\"RGB\"))\n",
    "    mask = _topk_shap_segments(vals, img, k, **(slic_kw or {}))\n",
    "\n",
    "    masked = _mask_image(img, mask)\n",
    "    pert = _doc_from_img(masked, sample)\n",
    "\n",
    "    d = _predict_prob(predict, sample) - _predict_prob(predict, pert)\n",
    "    return d, Image.fromarray(masked)\n",
    "\n",
    "\n",
    "def shap_sufficiency(predict, sample, exp, k=10, slic_kw=None):\n",
    "    vals = exp.values.mean(2) if exp.values.ndim == 3 else exp.values\n",
    "    img = np.asarray(sample.image.convert(\"RGB\"))\n",
    "    keep = _topk_shap_segments(vals, img, k, **(slic_kw or {}))\n",
    "    mask = ~keep\n",
    "\n",
    "    masked = _mask_image(img, mask)\n",
    "    pert = _doc_from_img(masked, sample)\n",
    "\n",
    "    d = _predict_prob(predict, sample) - _predict_prob(predict, pert)\n",
    "    return d, Image.fromarray(masked)"
   ],
   "id": "2d30abe0eb4d9d09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DUMP",
   "id": "6e4fbee065c3c50f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Vision",
   "id": "c127c836e5e2c5d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vision_explainer = LimeVisionExplainer(\n",
    "    LLMV3,\n",
    "    LLMV3_encode,\n",
    "    batch_size=4,\n",
    "    label = 4\n",
    ")\n",
    "\n",
    "vision_vals = vision_explainer.explain(CLASH[0], num_samples=1000, num_features=200)"
   ],
   "id": "131299a5b49fd0f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"Explanation score (R²):\", vision_vals.score)",
   "id": "b89f4855a8646e4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "segment_weights = dict(vision_vals.local_exp[2])\n",
    "\n",
    "# Create weight map\n",
    "segments = vision_vals.segments\n",
    "weight_map = np.zeros_like(segments, dtype=float)\n",
    "for seg_id, weight in segment_weights.items():\n",
    "    weight_map[segments == seg_id] = weight\n",
    "\n",
    "# Define overlay function\n",
    "def lime_weight_to_overlay(image_np, weight_map, alpha=0.4):\n",
    "    max_weight = np.max(np.abs(weight_map))\n",
    "    if max_weight == 0:\n",
    "        max_weight = 1\n",
    "    normalized_weights = weight_map / max_weight\n",
    "    r = np.where(normalized_weights > 0, 255,\n",
    "                 np.where(normalized_weights < 0, 255 * (1 + normalized_weights), 255))\n",
    "    g = np.where(normalized_weights > 0, 255 * (1 - normalized_weights),\n",
    "                 np.where(normalized_weights < 0, 255 * (1 + normalized_weights), 255))\n",
    "    b = np.where(normalized_weights > 0, 255 * (1 - normalized_weights),\n",
    "                 np.where(normalized_weights < 0, 255, 255))\n",
    "    color_map = np.stack([r, g, b], axis=-1).astype(np.uint8)\n",
    "    overlay = ((1 - alpha) * image_np + alpha * color_map).clip(0, 255).astype(np.uint8)\n",
    "    return overlay\n",
    "\n",
    "# Apply overlay\n",
    "img_np = vision_vals.image\n",
    "heat_np = lime_weight_to_overlay(img_np, weight_map, alpha=0.4)\n",
    "\n",
    "# Display\n",
    "Image.fromarray(heat_np)"
   ],
   "id": "fdf81eb384b02765"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from PIL import Image",
   "id": "596fa0bb43f5c152"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "img_np, mask = vision_vals.get_image_and_mask(\n",
    "    label = 2,\n",
    "    positive_only=False,   # include negative weights too\n",
    "    num_features=30,\n",
    "    hide_rest=False\n",
    ")\n",
    "\n",
    "heat_np = lime_mask_to_overlay(img_np, mask, alpha=0.40)\n",
    "Image.fromarray(heat_np)"
   ],
   "id": "79f7128159e5f116"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Explanation score (R²):\", vision_vals.score)\n",
    "for word, weight in vision_vals:\n",
    "  print(f\"{word:10s} -> {weight:+.10f}\")"
   ],
   "id": "aada0b3b7b68b7b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "text_explainer = LimeTextExplainer(\n",
    "    LLMV3,\n",
    "    LLMV3_encode,\n",
    "    mask_token = LLMV3_proc.tokenizer.mask_token,\n",
    "    batch_size = 2,\n",
    "    kernel_width_factor = 0.75,\n",
    "    labels = [4]\n",
    ")"
   ],
   "id": "b23e9077323f4e52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "text_vals = text_explainer.explain(AGREE[0], align_boxes=True, num_samples=2000, num_features=30)",
   "id": "81b74c9cc990eec7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Explanation score (R²):\", text_vals.score)\n",
    "for word, weight in text_vals.as_list(label=4):\n",
    "  print(f\"{word:10s} -> {weight:+.10f}\")"
   ],
   "id": "f904d1469aef7caa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from vrdu_utils.utils import draw_lime_token_heatmap\n",
    "import re\n",
    "weights = {}\n",
    "for token, w in text_vals.as_list(label=4):\n",
    "    clean = re.sub(r\"=\\d+$\", \"\", token)     # drop '=number' suffix\n",
    "    weights[clean] = weights.get(clean, 0.0) + w\n",
    "\n",
    "draw_lime_token_heatmap(image = AGREE[0].image, words = AGREE[0].words, boxes = AGREE[0].bboxes, weights = weights, alpha = 0.25)"
   ],
   "id": "e16c253a194fbc8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fe_text = FidelityEvaluator(\n",
    "    model = LLMV3,\n",
    "    encode_fn = LLMV3_encode,\n",
    "    device = device,\n",
    "    mask_token = LLMV3_proc.tokenizer.mask_token\n",
    ")"
   ],
   "id": "bb9091774f2057d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(AGREE[0].words)",
   "id": "a5be8f22960d03df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "top20 = sorted(weights, key=weights.get, reverse=True)[: int(len(weights)*0.2)]\n",
    "\n",
    "print(\"LIME feature strings:\", top20[:10])\n",
    "print(\"First 20 words in sample:\", AGREE[0].words[:20])\n",
    "\n",
    "# quick overlap check\n",
    "print(\"Overlap size:\",\n",
    "      len({f.split('=')[0] for f in top20}.intersection(AGREE[0].words)))"
   ],
   "id": "44f3dfefe2cb014d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "orig = fe_text._get_prediction_function(AGREE[0].label)(AGREE[0])\n",
    "pert = fe_text._get_prediction_function(AGREE[0].label)(\n",
    "            DocSample(image=AGREE[0].image, words=[\"[UNK]\"]*len(AGREE[0].words),\n",
    "                      bboxes=AGREE[0].bboxes, ner_tags=AGREE[0].ner_tags, label=AGREE[0].label))\n",
    "print(f\"p_orig  = {orig:.6f}\")\n",
    "print(f\"p_allUNK= {pert:.6f}\")"
   ],
   "id": "277d66a78cab5458"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scores = fe_text.evaluate(\n",
    "    sample          = AGREE[0],\n",
    "    explanation     = weights,\n",
    "    top_k_fraction  = 0.2,          # use 20 % of the most important tokens\n",
    ")\n",
    "print(scores)\n"
   ],
   "id": "7b14e4d40ae3541f"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
